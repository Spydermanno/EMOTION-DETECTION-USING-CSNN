{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# print(torch.version.cuda)\n",
    "torch.cuda.is_available()\n",
    "# print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "\n",
    "# Load FER2013 dataset\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Grayscale(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Change brightness and contrast\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),  # Add small shifts\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='./dataset/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root='./dataset/test', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSNN(nn.Module):\n",
    "    def __init__(self,beta=0.9, slope=25):\n",
    "        super(CSNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with 12 filters in first layer, 64 filters in second, etc.\n",
    "        self.conv1 = nn.Conv2d(1, 32,5)  # Input is 1 channel (grayscale)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64,5)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Adjusted for smaller output size after convolution and pooling\n",
    "        self.fc1 = nn.Linear(9 *9, 128)\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout3 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 7)  # 7 classes for FER2013 emotions\n",
    "        self.lif4 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        \n",
    "        self.timesteps = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        # Forward pass through convolution and pooling layers\n",
    "        cur1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(cur1, 2)\n",
    "        spk1, mem1 = self.lif1(pool1, mem1)\n",
    "\n",
    "        cur2 = self.conv2(spk1)\n",
    "        pool2 = F.max_pool2d(cur2, 2)\n",
    "        spk2, mem2 = self.lif2(pool2, mem2)\n",
    "        \n",
    "        cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        cur4 = self.fc2(spk3)\n",
    "        spk4, mem4 = self.lif4(cur4, mem4)\n",
    "        \n",
    "        return spk4, mem4\n",
    "\n",
    "        # spk_out = None\n",
    "        # for t in range(self.timesteps):\n",
    "        #     cur_input = x[t]\n",
    "\n",
    "        #     # Layer 1: Conv + Pool + LIF\n",
    "        #     cur1 = self.conv1(cur_input)\n",
    "        #     pool1 = F.max_pool2d(cur1, 2)\n",
    "        #     spk1, mem1 = self.lif1(pool1, mem1)\n",
    "\n",
    "        #     # Layer 2: Conv + Pool + LIF\n",
    "        #     cur2 = self.conv2(spk1)\n",
    "        #     pool2 = F.max_pool2d(cur2, 2)\n",
    "        #     spk2, mem2 = self.lif2(pool2, mem2)\n",
    "\n",
    "        #     # Layer 3: Flatten + FC + LIF\n",
    "        #     cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n",
    "        #     spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        #     # Layer 4: FC + LIF\n",
    "        #     cur4 = self.fc2(spk3)\n",
    "        #     spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "        #     # Output layer\n",
    "        #     out = self.fc3(spk4)\n",
    "        #     spk_out = out if spk_out is None else spk_out + out\n",
    "\n",
    "        # # Average the output over timesteps\n",
    "        # return spk_out / self.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CSNN(beta=0.9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original beta = 0.9 lr = 0.0001 epochs = 100 accuracy = 49.33% loss < 0.9\n",
    "#2nd try - beta = 0.99 lr = 0.0001 epochs = 20 accuracy = 35.64% loss 1.6463\n",
    "# 3nd try - beta = 0.95 lr = 0.0001 epochs = 20 loss 1.6640\n",
    "# 5nd try - beta = 0.9 lr = 0.001 epochs = 20 accuracy = 28.66% loss 1.7209\n",
    "# 6nd try - beta = 0.9 lr = 0.0001 epochs = 20 accuracy = 35.62% loss 1.6436\n",
    "# 7nd try - beta = 0.8 lr = 0.0001 epochs = 20 accuracy = 33.92% loss 1.6602\n",
    "# 8nd try - beta = 0.8 lr = 0.001 epochs = 20 loss 1.7313\n",
    "# 9nd try - beta = 0.7 lr = 0.0001 epochs = 20 loss 1.6574\n",
    "# 10nd try - beta = 0.5 lr = 0.0001 epochs = 20  loss 1.6670\n",
    "# 11nd try - beta = 0.65 lr = 0.0001 epochs = 20 loss 1.6332\n",
    "# 8nd try - beta = 0.7 lr = 0.0001 epochs = 20 accuracy = 33.92% loss 1.6574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_counts = {3: 7215, 4: 4965,\n",
    "                      5: 4830, 2: 4097, 0: 3995, 6: 3171, 1: 436}\n",
    "class_weights = torch.tensor([1 / train_class_counts[i]\n",
    "                             for i in range(7)]).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# loss_fn = SF.mse_count_loss(correct_rate=1.0,incorrect_rate=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  snn.utils.reset(net)\n",
    "  for step in range(data.size(0)):\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snntorch import spikegen\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "\n",
    "def train_snn(num_epochs):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # images = add_noise(images)\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # images = spikegen.rate(images,num_steps=10 )\n",
    "            # spk_rec,_= net(images)\n",
    "            # spk_rec,_,_= net(images)\n",
    "\n",
    "            spk_rec= forward_pass(net, images)\n",
    "\n",
    "            # print(spk_rec.size(),epoch)\n",
    "\n",
    "            # spk_rec.squeeze(1)\n",
    "\n",
    "            # labels = labels.view(-1)\n",
    "\n",
    "            # print(spk_rec.size())\n",
    "            labels_onehot = F.one_hot(labels, num_classes=7).long()\n",
    "            loss = loss_fn(spk_rec, labels_onehot)\n",
    "\n",
    "\n",
    "            # Backward pass and optimization\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "train_snn(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = spikegen.rate(images, num_steps=10)\n",
    "            outputs, a, _ = net(images)\n",
    "            # outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'csnn2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  snn.utils.reset(net)\n",
    "  for step in range(data.size(0)):\n",
    "      out, spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "counter = 0\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "test_acc_hist = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        # Downsampling image from (128 x 128) to (32 x 32)\n",
    "        # data = nn.functional.interpolate(data, size=(48, 48))\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        # propagating one batch through the network and evaluating loss\n",
    "        # spk_rec = forward_pass(net, data)\n",
    "        data = spikegen.rate(data,num_steps=10)\n",
    "        spk_rec = net(data)\n",
    "        # targets_one_hot = F.one_hot(targets, num_classes=7).long()\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "\n",
    "        # print metrics every so often\n",
    "        if counter % 16 == 0:\n",
    "          print(\n",
    "              f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "          print(f\"Train Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "          correct = 0\n",
    "          total = 0\n",
    "\n",
    "          for i, (data, targets) in enumerate(iter(test_loader)):\n",
    "            # data = nn.functional.interpolate(data, size=(48,48))\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            data = spikegen.rate(data,num_steps=10)\n",
    "            spk_rec = net(data)\n",
    "            # spk_rec = forward_pass(net, data)\n",
    "            correct += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "\n",
    "          test_acc = (correct/total) * 100\n",
    "          test_acc_hist.append(test_acc)\n",
    "          print(f\"========== Test Set Accuracy: {test_acc:.2f}% ==========\\n\")\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(num_epochs, train_loader, val_loader, timesteps):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        net.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Rate encoding for training\n",
    "            spike_trains = spikegen.rate(images, timesteps)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(spike_trains)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "        # acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        # acc_hist.append(acc)\n",
    "\n",
    "        # Validation Phase\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Rate encoding for validation\n",
    "                spike_trains = spikegen.rate(images, timesteps)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = net(spike_trains)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snn(num_epochs, train_loader, test_loader, timesteps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
